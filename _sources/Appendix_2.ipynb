{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 2: Local Code\n",
    "\n",
    "To run computer vision software with your own camera, you should install software on your computer:\n",
    "\n",
    "* Python\n",
    "* OpenCV\n",
    "* Dlib\n",
    "* Mediapipe\n",
    "\n",
    "You can then run the code in the following cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook: computer_vision4.ipynb\n",
    "# Detect 68 landmarks on a face\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "params = {'figure.figsize':(12,6), # These are plot parameters\n",
    "         'xtick.labelsize': 16,\n",
    "         'ytick.labelsize':16,\n",
    "         'axes.titlesize':18,\n",
    "         'axes.labelsize':18,\n",
    "         'lines.markersize':4,\n",
    "         'legend.fontsize': 20}\n",
    "matplotlib.rcParams.update(params)\n",
    "import math\n",
    "import cv2\n",
    "import dlib\n",
    "\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture the image from the webcam\n",
    "    ret, image = cap.read()\n",
    "    # Convert the image color to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Detect the face\n",
    "    rects = detector(gray, 1)\n",
    "    # Detect landmarks for each face\n",
    "    for rect in rects:\n",
    "        # Get the landmark points\n",
    "        shape = predictor(gray, rect)\n",
    "\t# Convert it to the NumPy Array\n",
    "        shape_np = np.zeros((68, 2), dtype=\"int\")\n",
    "        for i in range(0, 68):\n",
    "            shape_np[i] = (shape.part(i).x, shape.part(i).y)\n",
    "        shape = shape_np\n",
    "\n",
    "        # Display the landmarks\n",
    "        for i, (x, y) in enumerate(shape):\n",
    "\t    # Draw the circle to mark the keypoint \n",
    "            cv2.circle(image, (x, y), 1, (255, 255, 0), -1)\n",
    "\t\t\n",
    "    # Display the image\n",
    "    cv2.imshow('Landmark Detection', image)\n",
    "\n",
    "    # Press the escape button to terminate the code\n",
    "    if cv2.waitKey(10) == 27:\n",
    "        break\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook: computer_vision5.ipynb\n",
    "# Detect yawning\n",
    "\n",
    "from scipy.spatial import distance\n",
    "import cv2\n",
    "import imutils\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "\tA = distance.euclidean(eye[1], eye[5])\n",
    "\tB = distance.euclidean(eye[2], eye[4])\n",
    "\tC = distance.euclidean(eye[0], eye[3])\n",
    "\tear = (A + B) / (2.0 * C)\n",
    "\treturn ear\n",
    "\n",
    "thresh = 0.25\n",
    "frame_check = 20\n",
    "detect = dlib.get_frontal_face_detector()\n",
    "predict = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "flag=0\n",
    "while True:\n",
    "\tret, frame=cap.read()\n",
    "\tframe = imutils.resize(frame, width=780)\n",
    "\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\tsubjects = detect(gray, 0)\n",
    "\tfor subject in subjects:\n",
    "\t\tshape = predict(gray, subject)\n",
    "\t\tshape = face_utils.shape_to_np(shape)#converting to NumPy Array\n",
    "\t\tleftEye = shape[lStart:lEnd]\n",
    "\t\trightEye = shape[rStart:rEnd]\n",
    "\t\tleftEAR = eye_aspect_ratio(leftEye)\n",
    "\t\trightEAR = eye_aspect_ratio(rightEye)\n",
    "\t\tear = (leftEAR + rightEAR) / 2.0\n",
    "\t\tleftEyeHull = cv2.convexHull(leftEye)\n",
    "\t\trightEyeHull = cv2.convexHull(rightEye)\n",
    "\t\tcv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
    "\t\tcv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
    "\t\tif ear < thresh:\n",
    "\t\t\tflag += 1\n",
    "\t\t\t#print (flag)\n",
    "\t\t\tif flag >= frame_check:\n",
    "\t\t\t\tcv2.putText(frame, \"****************ALERT!****************\", (10, 30),\n",
    "\t\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\t\t\t\tcv2.putText(frame, \"****************ALERT!****************\", (10,325),\n",
    "\t\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\t\telse:\n",
    "\t\t\tflag = 0\n",
    "\tcv2.imshow(\"Frame\", frame)\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tcv2.destroyAllWindows()\n",
    "\t\tcap.release()\n",
    "\t\tbreak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================#\n",
    "#    Drowsiness Detection   #\n",
    "#   with Python and OpenCV  #\n",
    "#===========================#\n",
    "#    Konstantinos Thanos    #\n",
    "#    Mathematician, Msc     #\n",
    "#===========================#\n",
    "\n",
    "# Import packages\n",
    "from scipy.spatial import distance as dist\n",
    "import numpy as np\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "path = 'https://raw.githubusercontent.com/jeanwalrand/Notes_on_AI/main/'\n",
    "\n",
    "def getImage(name):\n",
    "    response = requests.get(path+name)\n",
    "    im1 = Image.open(BytesIO(response.content))\n",
    "    im2 = np.array(im1)\n",
    "    return im2\n",
    "\n",
    "im = cv2.imread('coffee_break.png')\n",
    "\n",
    "width = 100\n",
    "height = 100\n",
    "dim = (width, height)\n",
    " \n",
    "# resize image\n",
    "image = cv2.resize(im, dim, interpolation = cv2.INTER_AREA)\n",
    " \n",
    "\n",
    "'''\n",
    "Eye Aspect Ratio (E.A.R.)\n",
    "Function to calculate eye aspect ratio as in paper :\n",
    "\"Real-Time Eye Blink Detection using Facial Landmarks [Soukupova, Cech]\"             \n",
    "Landmarks |   0  1  2  3  4  5\n",
    " Left Eye : [36,37,38,39,40,41]\n",
    "Right Eye : [42,43,44,45,46,47]\n",
    "'''\n",
    "def eye_aspect_ratio(eye):\n",
    "     # Vertical distances\n",
    "     dist1 = dist.euclidean(eye[1], eye[5]) # P2-P6\n",
    "     dist2 = dist.euclidean(eye[2], eye[4]) # P3-P5\n",
    "     # Horiontal distance\n",
    "     dist3 = dist.euclidean(eye[0], eye[3]) # P1-P4\n",
    "\n",
    "     # Eye Aspect Ratio (E.A.R.)\n",
    "     ear = (dist1 + dist2) / (2.0 * dist3)\n",
    "\n",
    "     return ear\n",
    "\n",
    "\n",
    "'''\n",
    "Lips Aspect Ratio (L.A.R.)\n",
    "Function to calculate lips aspect ratio in the same way as in E.A.R.\n",
    "Landmarks |   0  1  2  3  4  5  6  7\n",
    "     Lips : [60,61,62,63,64,65,66,67]\n",
    "'''\n",
    "def lips_aspect_ratio(lips):\n",
    "     # Vertical distance\n",
    "     dist1 = dist.euclidean(lips[2], lips[6]) # L3-L7\n",
    "     # Horiontal distance\n",
    "     dist2 = dist.euclidean(lips[0], lips[4]) # L1-L5\n",
    "\n",
    "     # Lips Aspect Ratio (L.A.R.)\n",
    "     lar = float(dist1/dist2)\n",
    "\n",
    "     return lar\n",
    "    \n",
    "'''\n",
    "Facial Landmarks for any face part\n",
    "Function to calculate facial landmark point coordinates (x,y),\n",
    "draw them on frame and return a numpy array with the corresponding points\n",
    "'''\n",
    "def draw_landmarks(face_part, landmarks):\n",
    "     landmarks_list = []\n",
    "     for point in face_part:\n",
    "          x, y = landmarks.part(point).x, landmarks.part(point).y\n",
    "          landmarks_list.append([x,y])\n",
    "          cv2.circle(frame, (x,y), 2, (0,0,255), -1)\n",
    "          \n",
    "     return np.array(landmarks_list)\n",
    "\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# while True:\n",
    "#     # Capture the image from the webcam\n",
    "#     ret, image = cap.read()\n",
    "#     # Convert the image color to grayscale\n",
    "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#     # Detect the face\n",
    "#     rects = detector(gray, 1)\n",
    "#     # Detect landmarks for each face\n",
    "#     for rect in rects:\n",
    "#         # Get the landmark points\n",
    "#         shape = predictor(gray, rect)\n",
    "# \t# Convert it to the NumPy Array\n",
    "#         shape_np = np.zeros((68, 2), dtype=\"int\")\n",
    "#         for i in range(0, 68):\n",
    "#             shape_np[i] = (shape.part(i).x, shape.part(i).y)\n",
    "#         shape = shape_np\n",
    "\n",
    "#         # Display the landmarks\n",
    "#         for i, (x, y) in enumerate(shape):\n",
    "# \t    # Draw the circle to mark the keypoint \n",
    "#             cv2.circle(image, (x, y), 1, (0, 0, 255), -1)\n",
    "\t\t\n",
    "#     # Display the image\n",
    "#     cv2.imshow('Landmark Detection', image)\n",
    "\n",
    "#     # Press the escape button to terminate the code\n",
    "#     if cv2.waitKey(10) == 27:\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "\n",
    "\n",
    "\n",
    "# Text settings\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 0.7\n",
    "\n",
    "# Initializations\n",
    "frames = 0\n",
    "\n",
    "# ear & lar, threshold values\n",
    "ear_thresh = 0.15\n",
    "lar_thresh = 0.3\n",
    "\n",
    "# Blink initializations\n",
    "blink_counter, total_blinks = 0, 0\n",
    "# Yawn initializations\n",
    "yawn_counter, total_yawns = 0, 0\n",
    "\n",
    "while True:\n",
    "     _, frame = cap.read()\n",
    "     frame = cv2.flip(frame, 1) # May not be necessary\n",
    "     h, w = frame.shape[: 2]    # Height and Width of frame\n",
    "     \n",
    "     frames += 1\n",
    "\n",
    "     # Grayscale\n",
    "     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "     # Detect faces in the gray frame\n",
    "     faces = detector(gray, 0)\n",
    "\n",
    "     # Loop through each face\n",
    "     for face in faces:\n",
    "          # Determine facial landmarks\n",
    "          facial_landmarks = predictor(gray, face)\n",
    "\n",
    "          # Landmark indexes for eyes and lips\n",
    "          left_eye = [36,37,38,39,40,41]\n",
    "          right_eye = [42,43,44,45,46,47]\n",
    "          \n",
    "          lips = [60,61,62,63,64,65,66,67]\n",
    "\n",
    "          # Convert to numpy array the above lists and\n",
    "          # draw the corresponding facial landmark points on frame\n",
    "          left_eye_points = draw_landmarks(left_eye, facial_landmarks)\n",
    "          right_eye_points = draw_landmarks(right_eye, facial_landmarks)\n",
    "\n",
    "          lips_points = draw_landmarks(lips, facial_landmarks)\n",
    "\n",
    "          # Find and draw the convex hulls of left and right eye, and lips\n",
    "          left_eye_hull = cv2.convexHull(left_eye_points)      \n",
    "          cv2.drawContours(frame, [left_eye_hull], -1, (0, 255, 0), 1)\n",
    "          \n",
    "          right_eye_hull = cv2.convexHull(right_eye_points)\n",
    "          cv2.drawContours(frame, [right_eye_hull], -1, (0, 255, 0), 1)\n",
    "\n",
    "          lips_hull = cv2.convexHull(lips_points)\n",
    "          cv2.drawContours(frame, [lips_hull], -1, (0, 255, 0), 1)\n",
    "\n",
    "          # Calculate E.A.R. and L.A.R.\n",
    "          left_ear = eye_aspect_ratio(left_eye_points)    # Left eye aspect ratio\n",
    "          right_ear =  eye_aspect_ratio(right_eye_points) # Right eye aspect ratio\n",
    "          ear = (left_ear + right_ear) / 2.0              # Average eye aspect ratio\n",
    "          cv2.putText(frame, \"E.A.R. : {:.2f}\".format(ear), (10,30), font, font_scale, (0,0,255), 2)\n",
    "\n",
    "          lar = lips_aspect_ratio(lips_points) # Lips aspect ratio\n",
    "          cv2.putText(frame, \"L.A.R. : {:.2f}\".format(lar), (10,90), font, font_scale, (0,0,255), 2)\n",
    "\n",
    "          # Check for blinks or yawns\n",
    "          # BLINK\n",
    "          if ear < ear_thresh:\n",
    "               blink_counter += 1\n",
    "          else:\n",
    "               if blink_counter > 3:\n",
    "                    total_blinks += 1\n",
    "               blink_counter = 0\n",
    "          cv2.putText(frame, \"Blinks: {}\".format(total_blinks), (10, 50), font, font_scale, (0, 0, 255), 2)\n",
    "\n",
    "          # YAWN\n",
    "          if lar > lar_thresh:\n",
    "               yawn_counter += 1\n",
    "          else:\n",
    "               if yawn_counter > 1:\n",
    "                    total_yawns += 1\n",
    "               yawn_counter = 0\n",
    "          cv2.putText(frame, \"Yawns: {}\".format(total_yawns), (10, 110), font, font_scale, (0, 0, 255), 2)\n",
    "          \n",
    "          # Drowsiness Detection\n",
    "          if total_yawns > 2 or total_blinks > 3:\n",
    "               frame[20:120, w-120:w-20] = image # Show coffee break image\n",
    "               cv2.putText(frame, \"ALERT\", (w-120, 160), font, 1.2, (0, 0, 255), 4)\n",
    "\n",
    "     cv2.imshow('Frame', frame)\n",
    "     key = cv2.waitKey(1)\n",
    "     if key == 27:\n",
    "          break\n",
    "     if key==ord('r') or key==ord('R'):\n",
    "          total_blinks, total_yawns = 0, 0 # Reset calculations by pressing 'r' or 'R'\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook: computer_vision7.ipynb\n",
    "# Fit mesh on face using mediapipe\n",
    "\n",
    "# IMPORTING LIBRARIES  --> pip install mediapipe\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# INITIALIZING OBJECTS\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=5, circle_radius=2)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# DETECT THE FACE LANDMARKS\n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "  while True:\n",
    "    success, image = cap.read()\n",
    "\n",
    "    # Flip the image horizontally and convert the color space from BGR to RGB\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # To improve performance\n",
    "    image.flags.writeable = False\n",
    "    \n",
    "    # Detect the face landmarks\n",
    "    results = face_mesh.process(image)\n",
    "\n",
    "    # To improve performance\n",
    "    image.flags.writeable = True\n",
    "\n",
    "    # Convert back to the BGR color space\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Draw the face mesh annotations on the image.\n",
    "    if results.multi_face_landmarks:\n",
    "      for face_landmarks in results.multi_face_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image=image,\n",
    "            landmark_list=face_landmarks,\n",
    "            connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp_drawing_styles\n",
    "            .get_default_face_mesh_tesselation_style())\n",
    "\n",
    "    # Display the image\n",
    "    cv2.imshow('MediaPipe FaceMesh', image)\n",
    "    \n",
    "    # Terminate the process\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "\n",
    "cap.release()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
